#!/usr/bin/env bash
# æ–‡æœ¬äººå‘³è‡ªæŸ¥ï¼ˆç¦»çº¿ï¼‰ï¼šè¿æ¥è¯/ç©ºè¯å¯†åº¦ã€å¥é•¿ç»Ÿè®¡ã€æŠ½è±¡è¯å¯†åº¦

set -e

SCRIPT_DIR=$(dirname "$0")
source "$SCRIPT_DIR/common.sh"

PROJECT_ROOT=$(get_project_root)

FILE_PATH="$1"
if [ -z "$FILE_PATH" ] || [ ! -f "$FILE_PATH" ]; then
  echo "ç”¨æ³•: scripts/bash/text-audit.sh <file>"
  exit 1
fi

# é€‰æ‹©é…ç½®ï¼šä¼˜å…ˆé¡¹ç›® spec/knowledgeï¼Œå…¶æ¬¡ .specify/templates/knowledge
CFG_PROJECT="$PROJECT_ROOT/spec/knowledge/audit-config.json"
CFG_TEMPLATE="$PROJECT_ROOT/.specify/templates/knowledge/audit-config.json"
if [ -f "$CFG_PROJECT" ]; then
  CFG="$CFG_PROJECT"
elif [ -f "$CFG_TEMPLATE" ]; then
  CFG="$CFG_TEMPLATE"
else
  CFG=""
fi

python3 - "$FILE_PATH" "$CFG" << 'PY'
import json, re, sys, os, math

path = sys.argv[1]
cfg_path = sys.argv[2] if len(sys.argv) > 2 else ''

text = open(path, 'r', encoding='utf-8', errors='ignore').read()

default_cfg = {
  "connector_phrases": ["é¦–å…ˆ","å…¶æ¬¡","å†æ¬¡","ç„¶å","ç„¶è€Œ","æ€»è€Œè¨€ä¹‹","ç»¼ä¸Šæ‰€è¿°","åœ¨æŸç§ç¨‹åº¦","ä¼—æ‰€å‘¨çŸ¥","åœ¨å½“ä¸‹","éšç€"],
  "empty_phrases": ["å¹¿æ³›å…³æ³¨","å¼•å‘çƒ­è®®","å½±å“æ·±è¿œ","å…·æœ‰é‡è¦æ„ä¹‰","æœ‰æ•ˆæå‡","å…·æœ‰ä¸€å®šçš„æŒ‡å¯¼æ„ä¹‰","å€¼å¾—æˆ‘ä»¬æ€è€ƒ"],
  "cliche_pairs": [],
  "sentence_length": {"max_run_long":4, "max_run_short":5, "short_threshold":12, "long_threshold":35},
  "abstract_nouns": ["ä»·å€¼","æ„ä¹‰","è®¤çŸ¥","ä½“ç³»","æ¨¡å¼","è·¯å¾„","æ–¹æ³•è®º","è¶‹åŠ¿"],
  "min_concrete_details": 3
}

cfg = default_cfg
if cfg_path and os.path.exists(cfg_path):
  try:
    with open(cfg_path, 'r', encoding='utf-8') as f:
      loaded = json.load(f)
      cfg.update(loaded)
  except Exception:
    pass

def count_occurrences(text, phrases):
  res = {}
  for p in phrases:
    if not p: continue
    res[p] = len(re.findall(re.escape(p), text))
  return res

def split_sentences(t):
  parts = re.split(r'[ã€‚ï¼ï¼Ÿ!?\n]+', t)
  return [s.strip() for s in parts if s.strip()]

def sentence_lengths(sents):
  lens = [len(s) for s in sents]
  if not lens:
    return lens, 0, 0
  avg = sum(lens)/len(lens)
  var = sum((x-avg)**2 for x in lens)/len(lens)
  return lens, avg, math.sqrt(var)

def runs(lens, short_th, long_th):
  run_short = 0; run_long = 0
  max_run_short = 0; max_run_long = 0
  marks = []
  for i, L in enumerate(lens):
    if L <= short_th:
      run_short += 1; max_run_short = max(max_run_short, run_short); run_long = 0
    elif L >= long_th:
      run_long += 1; max_run_long = max(max_run_long, run_long); run_short = 0
    else:
      run_short = 0; run_long = 0
  return max_run_short, max_run_long

def abstract_density(sent, abstract_words):
  cnt = sum(len(re.findall(re.escape(w), sent)) for w in abstract_words)
  return cnt

connectors = count_occurrences(text, cfg["connector_phrases"])
empties = count_occurrences(text, cfg["empty_phrases"])
sents = split_sentences(text)
lens, avg, std = sentence_lengths(sents)
mx_run_short, mx_run_long = runs(lens, cfg["sentence_length"]["short_threshold"], cfg["sentence_length"]["long_threshold"])

abstract_scores = [(i, abstract_density(s, cfg["abstract_nouns"])) for i, s in enumerate(sents)]
abstract_scores.sort(key=lambda x: x[1], reverse=True)
abstract_top = [ (i, sents[i]) for i,score in abstract_scores[:5] if score>=2 ]

total_chars = len(text)
def ratio(count):
  return (count / max(1,total_chars)) * 1000

print("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
print("ğŸ“Š ç¦»çº¿æ–‡æœ¬äººå‘³è‡ªæŸ¥æŠ¥å‘Š")
print(f"æ–‡ä»¶: {os.path.basename(path)}  å­—ç¬¦æ•°: {total_chars}")
print("")
print("è¿æ¥è¯å¯†åº¦ï¼ˆæ¯åƒå­—å‡ºç°æ¬¡æ•°ï¼‰")
total_conn = sum(connectors.values())
print(f"  æ€»è®¡: {total_conn}  | æ¯”ç‡: {ratio(total_conn):.2f}")
for k,v in sorted(connectors.items(), key=lambda x: -x[1])[:10]:
  if v>0: print(f"  - {k}: {v}")

print("")
print("ç©ºè¯/å¥—è¯è®¡æ•°")
total_emp = sum(empties.values())
print(f"  æ€»è®¡: {total_emp}  | æ¯”ç‡: {ratio(total_emp):.2f}")
for k,v in sorted(empties.items(), key=lambda x: -x[1])[:10]:
  if v>0: print(f"  - {k}: {v}")

print("")
print("å¥é•¿ç»Ÿè®¡")
print(f"  å¥å­æ•°: {len(lens)}  | å¹³å‡: {avg:.1f}  | æ ‡å‡†å·®: {std:.1f}")
print(f"  è¿ç»­çŸ­å¥æœ€å¤§: {mx_run_short} (é˜ˆå€¼ {cfg['sentence_length']['max_run_short']})")
print(f"  è¿ç»­é•¿å¥æœ€å¤§: {mx_run_long} (é˜ˆå€¼ {cfg['sentence_length']['max_run_long']})")

print("")
print("æŠ½è±¡è¿‡è½½ï¼ˆç¤ºä¾‹æ®µï¼Œâ‰¥2 æŠ½è±¡è¯ï¼‰")
if abstract_top:
  for idx, s in abstract_top:
    snippet = s[:80] + ("â€¦" if len(s)>80 else "")
    print(f"  - ç¬¬{idx+1}å¥: {snippet}")
else:
  print("  æ— æ˜¾è‘—æŠ½è±¡è¿‡è½½ç‰‡æ®µ")

print("")
print("å»ºè®®")
print("  - ç”¨å…·ä½“åŠ¨ä½œ/å™¨ç‰©/æ°”å‘³æ›¿ä»£ç©ºè¯ä¸æŠ½è±¡åè¯")
print("  - æ‰“æ–­é•¿é•¿ä¸²å¥å­ï¼›åˆå¹¶è¿‡å¤šçš„çŸ­å¥ä»¥å½¢æˆèµ·ä¼")
print("  - å¤æŸ¥è¿æ¥è¯æ˜¯å¦å¯åˆ é™¤æˆ–è‡ªç„¶è¿‡æ¸¡")
print("  - å†™å‰å…ˆåˆ—3ä¸ªç”Ÿæ´»ç»†èŠ‚ä½œä¸ºé”šç‚¹")
print("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
PY

